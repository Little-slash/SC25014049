---
title: "Homework-2025.11.17"
author: "By SC25014049"
date: "2025/11/17"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to R-package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Question1 
Refer to the Bayesian prediction application in Example 11.3, with the Geometric(p) survival model. Prove that the derived parameter $\phi(p)=\frac{p}{1-p}$, does not depend on attained age of the individual in this model.(This is not true in general for other models.)  

example 11.3:
$$
Pr(T=k)=p^k(1-p)，k=0,1,2,,,
$$ 

## Answer1

有例11.3可知
在几何生存模型中，生存时间 $T$ 服从参数为 $p$ 的几何分布，其概率质量函数为：
$$
Pr(T=k)=p^k(1-p)，k=0,1,2,,,
$$
  
派生参数 $\phi(p) = \frac{p}{1-p}$ 表示期望寿命（平均剩余寿命）

假设个体已存活至年龄 $t$（即 $T \geq t$），定义剩余寿命 $S = T - t$。需计算条件概率 $\Pr(S = s \mid T \geq t)$：
$$
Pr(S=s|T>=t) = Pr(T=t+s|T>=t)=\frac{Pr(T=t+s)}{Pr(T>=t)}
$$
则:
$$
Pr(S=s|T>=t) =\frac{p^{t+s}(1-p)}{p^t}=p^s(1-p)
$$
该结果正是几何分布 $\Pr(S = s) = p^s (1 - p)$，表明剩余寿命 $S$ 与原始寿命 $T$ 同分布，且独立于 $t$。

因此,$\phi(p)$ 与已存活年龄 $t$ 无关。

通过随机生成验证如下:
```{r,echo=FALSE}

p <- 0.3
t <- 5
n <- 10000  

set.seed(123)
T <- rgeom(n, prob = 1 - p)

T_t <- T[T >= t]
S <- T_t - t 

mean_S <- mean(S)
phi_p <- p / (1 - p)


cat("模拟剩余寿命均值:", round(mean_S, 4), "\n")
cat("理论phi(p):", round(phi_p, 4), "\n")
cat("差异:", round(mean_S - phi_p, 4))
```


## Question2
Use the simplex algorithm to solve the following problem:Minimize 4x+2y+9z subject to
$$
2x+y+z<=2\\
x-y+3z<=3\\
x>=0,y>=0,z>=0.
$$

## Answer2
参考Example中的代码，利用单纯形法可得如下结果，
```{r,echo=FALSE}

library(boot)
a <- c(-4, -2, -9)
A1 <- rbind(c(2, 1, 1),   
            c(1, -1, 3))

b1 <- c(2, 3)

res <- simplex(a = a, A1 = A1, b1 = b1, maxi = TRUE)

cat("最优解:\n")
cat("x =", res$soln[1], "\n")
cat("y =", res$soln[2], "\n") 
cat("z =", res$soln[3], "\n")
cat("最优目标函数值:", -res$value, "(最小化形式)\n")  # 注意转换回最小化问题
```
则最优目标函数值为0。

## Quetsion3

设X1,,,X_i~Exp($\lambda$)
因为某种原因，只知道$X_i$落在区间$(u_i,v_i)$其中 u<v: 非随机---区间删失数据

1)试分别直接极大化观测数据的似然函数、与采用EM算法求解$\lambda$的MLE。证明EM算法收敛于观测观测数据的MLE，且收敛有线性速度

2)设$(u_i,v_i)，i=1,,,,n(=10)的观测值分别为(11,12),(8,9),(27,28),(13,14),(16,17),(0,1),(23,24),(10,11),(24,25),(2,3)$
试分别编程实现上述两种算法，并比较结果

3)数值比较两个算法的收敛速度
设置公共的初始值。

## Answer3

### 1.直接求解极大化观测数据的似然函数、与采用EM算法求解$\lambda$的MLE

对于每个观测值 $X_i$，已知其落在区间 $(u_i, v_i)$ 内，其概率为：
$$
p(u_i<  X_i<v_i) = F(v_i,\lambda)-F(u_i,\lambda)=e^{-\lambda u_i}-e^{-\lambda v_i}
$$

其似然函数为：
$$
L(\lambda)=\prod_{i=1}^{n}e^{-\lambda u_i}-e^{-\lambda v_i}
$$
对数似然函数:
$$
l(\lambda)=\sum_{i=1}^{n}ln(e^{-\lambda u_i}-e^{-\lambda v_i})
$$

则极大似然估计通过求导:
$$
\frac{\mathrm{d} l(\lambda)}{\mathrm{d} \lambda}=\sum_{i=1}^{n}\frac{-u_ie^{-\lambda u_i}+v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}=0
$$

单调性：由EM算法的定义，每次迭代后观测数据的似然函数值不降，即 $L(λ^{(t+1)}) ≥ L(λ^{(t)})$,对于指数分布，当λ→0+时，似然函数趋于0（因为每个区间概率趋于0）；当λ趋向于无穷时，似然函数也趋于0（因为区间概率趋于0）。因此，似然函数在$(0,\infty)$上有最大值，且由区间删失数据，似然函数连续，因此存在最大值点。由于EM算法每次迭代不降低似然函数，且似然函数有上界，因此似然函数序列收敛。在指数分布且缺失数据为单调缺失的情况下，EM算法产生的参数序列收敛到似然函数的MLE,
M步中：
M步：求导并令导数为0：
$$
M(\lambda) = \frac{n}{\sum_{i=1}^nE[X_i|u_i<X_i<v_i,\lambda] }
$$
$\lambda^{(t+1)} = M(\lambda^{(t)})$
在MLE $\lambda^*$ 附近对 $M(\lambda)$ 进行泰勒展开：
$$
M(\lambda) = M(\lambda^{*})+M'(\lambda^{*})(\lambda-\lambda^*)+O((\lambda-\lambda^*)^2)\\
\lambda^{(t+1)}-\lambda^{*}=  M'(\lambda^{*})(\lambda^{t}-\lambda^*)
$$

收敛率由 $|M'(\lambda^*)|$ 决定。对于指数分布和区间删失数据

$$
M'(\lambda^*)=\frac{I_m(\lambda^*)}{I_c(\lambda^*)}
$$

EM算法以线性速度收敛。

### 2.采用两种方法求解
可得以下结果：

```{r,echo=FALSE}
u <- c(11,8,27,13,16,0,23,10,24,2)
v <- c(12,9,28,14,17,1,24,11,25,3)
n <- length(u)

l0 <- 0.1

# 直接极大化似然函数
dir_mle <- function(l, u, v) {
  ll <- sum(log(exp(-l*u) - exp(-l*v)))
  return(-ll)
}

res <- optimize(dir_mle, c(0.001, 1), u=u, v=v)
l_direct <- res$minimum

# EM算法
em_alg <- function(u, v, l0, tol=1e-8, max_iter=1000) {
  l_curr <- l0
  l_hist <- numeric(max_iter)
  l_hist[1] <- l0
  
  for (i in 2:max_iter) {
    num <- u*exp(-l_curr*u) - v*exp(-l_curr*v)
    den <- exp(-l_curr*u) - exp(-l_curr*v)
    ex <- 1/l_curr + num/den
    

    l_new <- n / sum(ex)
    
    l_hist[i] <- l_new
    
 
    if (abs(l_new - l_curr) < tol) {
      l_hist <- l_hist[1:i]
      break
    }
    l_curr <- l_new
  }
  
  return(list(l_est = l_curr, l_hist = l_hist, iter = i-1))
}

em_res <- em_alg(u, v, l0)
cat("直接法:", l_direct, "\n")
cat("EM算法:", em_res$l_est, "\n")
cat("相对差异:", abs(l_direct - em_res$l_est)/l_direct, "\n")
```
由上述结果可以证明第一题的结论，两种方法得到的估计值非常接近。

### 3.比较收敛速度

同时绘制收敛曲线如下：

```{r,echo=FALSE}

l_em <- em_res$l_hist
err_em <- abs(l_em - l_direct)

ratios <- err_em[2:length(err_em)] / err_em[1:(length(err_em)-1)]
avg_ratio <- mean(ratios[!is.na(ratios) & !is.infinite(ratios)])

# 绘制收敛过程
plot(1:length(l_em), l_em, type="b", xlab="迭代次数", ylab="λ估计值", 
     main="EM算法收敛过程", col="blue")
abline(h=l_direct, col="red", lty=2, lwd=2)
legend("topright", legend=c("EM算法", "直接法MLE"), 
       col=c("blue", "red"), lty=c(1,2))

# 绘制误差收敛
plot(1:length(err_em), log(err_em), type="b", 
     xlab="迭代次数", ylab="log(误差)", 
     main="EM算法收敛速度", col="darkgreen")

```

由上图可以直接看出EM算法是线性收敛到MLE。


```{r,echo=FALSE}

# 比较结果
cat("\n方法比较:\n")
cat("EM算法迭代次数:", em_res$iter, "\n")
```

且EM算法的迭代了三次稳定到MLE
